package oracleai;

import com.google.api.gax.rpc.ApiStreamObserver;
import com.google.api.gax.rpc.BidiStreamingCallable;
import com.google.cloud.speech.v1.*;
import com.google.protobuf.ByteString;
import org.springframework.stereotype.Component;
import org.springframework.web.socket.BinaryMessage;
import org.springframework.web.socket.CloseStatus;
import org.springframework.web.socket.WebSocketSession;
import org.springframework.web.socket.handler.BinaryWebSocketHandler;
import org.springframework.web.socket.TextMessage;


import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.concurrent.ConcurrentHashMap;

import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import oracleai.gcpspeech.*;


@Component
public class CustomWebSocketHandler extends BinaryWebSocketHandler {
    private static final ConcurrentHashMap<String, ApiStreamObserver<StreamingRecognizeRequest>> activeSessions = new ConcurrentHashMap<>();
    private static SpeechClient speechClient;

    static {
        try {
            speechClient = SpeechClient.create();
        } catch (IOException e) {
            throw new RuntimeException("Failed to initialize SpeechClient", e);
        }
    }

    @Override
    @Override
public void afterConnectionEstablished(WebSocketSession session) {
    System.out.println("‚úÖ WebSocket Connected: " + session.getId());

    ApiStreamObserver<StreamingRecognizeResponse> responseObserver = new ApiStreamObserver<>() {
        @Override
        public void onNext(StreamingRecognizeResponse response) {
            for (StreamingRecognitionResult result : response.getResultsList()) {
                if (result.getAlternativesCount() > 0) {
                    String transcript = result.getAlternatives(0).getTranscript().trim();
                    if (!transcript.isEmpty()) {
                        System.out.println("üìù Full API Response: " + response.toString());
                        System.out.println("üìù Transcription: " + transcript);

                        try {
                            session.sendMessage(new TextMessage(transcript));
                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }
            }
        }

        @Override
        public void onError(Throwable t) {
            System.err.println("‚ùå Google API Error: " + t.getMessage());
        }

        @Override
        public void onCompleted() {
            System.out.println("‚úÖ Streaming completed.");
        }
    };

    // ‚úÖ Start Google Speech API streaming
    BidiStreamingCallable<StreamingRecognizeRequest, StreamingRecognizeResponse> callable =
            speechClient.streamingRecognizeCallable();
    ApiStreamObserver<StreamingRecognizeRequest> requestObserver = callable.bidiStreamingCall(responseObserver);
    activeSessions.put(session.getId(), requestObserver);

    // ‚úÖ Adjust Speech Recognition Config
    RecognitionConfig config = RecognitionConfig.newBuilder()
            .setEncoding(RecognitionConfig.AudioEncoding.LINEAR16)
            .setSampleRateHertz(16000)
            .setLanguageCode("en-US")
            .setEnableAutomaticPunctuation(true)  // ‚úÖ Group words into phrases
            .setAudioChannelCount(1) // Force mono-channel
            .setEnableWordTimeOffsets(true) // Get timestamps for words
            .build();

        //     .setSpeechContexts(SpeechContext.newBuilder()
        // .addPhrases("AI Holo")
        // .addPhrases("Oracle Cloud")
        // .addPhrases("Speech Recognition")
        // .setBoost(15.0f) // Increase priority of these words
        // .build())


    StreamingRecognitionConfig streamingConfig = StreamingRecognitionConfig.newBuilder()
            .setConfig(config)
            .setInterimResults(true)
            .setSingleUtterance(false) // ‚úÖ Keep listening for long speech instead of stopping early
            .build();

    // ‚úÖ Send config to Google API
    requestObserver.onNext(StreamingRecognizeRequest.newBuilder()
            .setStreamingConfig(streamingConfig)
            .build());
}

    public void afterConnectionEstablished0(WebSocketSession session) {
        System.out.println("‚úÖ WebSocket Connected: " + session.getId());

        ApiStreamObserver<StreamingRecognizeResponse> responseObserver = new ApiStreamObserver<>() {

            @Override
            public void onNext(StreamingRecognizeResponse response) {
                System.out.println("üîç Full API Response: " + response);
            
                for (StreamingRecognitionResult result : response.getResultsList()) {
                    if (result.getAlternativesCount() > 0) {
                        String transcript = result.getAlternatives(0).getTranscript();
                        System.out.println("üìù Transcription: " + transcript);
            
                        if (!transcript.trim().isEmpty()) {
                            if (!transcript.trim().equals("")) {
                                System.out.println("üìù Transcription NOT EMPTY!!!!!!!!: " + transcript);
                                System.out.println("üìù Transcription NOT EMPTY!!!!!!!!: " + transcript);
                                System.out.println("üìù Transcription NOT EMPTY!!!!!!!!: " + transcript);
                                System.out.println("üìù Transcription NOT EMPTY!!!!!!!!: " + transcript);
                                System.out.println("üìù Transcription NOT EMPTY!!!!!!!!: " + transcript);
                                System.out.println("üìù Transcription NOT EMPTY!!!!!!!!: " + transcript);
                                System.out.println("üìù Transcription NOT EMPTY!!!!!!!!: " + transcript);
                                System.out.println("üìù Transcription NOT EMPTY!!!!!!!!: " + transcript);
                            }
                            try {
                                session.sendMessage(new TextMessage(transcript));
                            } catch (IOException e) {
                                e.printStackTrace();
                            }
                        } else {
                            System.out.println("‚ö†Ô∏è Empty transcription received. Possibly silence or unrecognized speech.");
                        }
                    } else {
                        System.out.println("‚ö†Ô∏è No recognition alternatives received.");
                    }
                }
            }
            

            @Override
            public void onError(Throwable t) {
                System.err.println("‚ùå Google API Error: " + t.getMessage());
            }

            @Override
            public void onCompleted() {
                System.out.println("‚úÖ Streaming completed.");
            }
        };

        // Start Google Speech API streaming
        BidiStreamingCallable<StreamingRecognizeRequest, StreamingRecognizeResponse> callable =
                speechClient.streamingRecognizeCallable();
        ApiStreamObserver<StreamingRecognizeRequest> requestObserver = callable.bidiStreamingCall(responseObserver);
        activeSessions.put(session.getId(), requestObserver);

        RecognitionConfig config0 = RecognitionConfig.newBuilder()
                .setEncoding(RecognitionConfig.AudioEncoding.LINEAR16) // Ensure correct encoding
                .setSampleRateHertz(16000) // Ensure correct sample rate
                .setLanguageCode("en-US")
                .setAudioChannelCount(1) // Force mono-channel
                .setEnableAutomaticPunctuation(false)
                .build();

                RecognitionConfig config = RecognitionConfig.newBuilder()
                .setEncoding(RecognitionConfig.AudioEncoding.LINEAR16)
                .setSampleRateHertz(16000)
                .setLanguageCode("en-US")
                .setAudioChannelCount(1)
                .setEnableAutomaticPunctuation(false)
                .setModel("latest_long")  // Use a more advanced model
//                 Google Speech-to-Text model:
// "latest_long" ‚Üí Good for long-form speech.
// "command_and_search" ‚Üí Good for short, command-like phrases.
// "default" ‚Üí General model.
                .build();
        
        

        StreamingRecognitionConfig streamingConfig = StreamingRecognitionConfig.newBuilder()
                .setConfig(config)
                .setInterimResults(true)
                .setSingleUtterance(false)
                .build();

        requestObserver.onNext(StreamingRecognizeRequest.newBuilder()
                .setStreamingConfig(streamingConfig)
                .build());
    }

    // @Override
    protected void handleBinaryMessage0(WebSocketSession session, BinaryMessage message) {
        ByteBuffer payload = message.getPayload();
        System.out.println("üîä Received audio data: " + payload.remaining() + " bytes");

        if (activeSessions.containsKey(session.getId())) {
            ApiStreamObserver<StreamingRecognizeRequest> requestObserver = activeSessions.get(session.getId());
            requestObserver.onNext(StreamingRecognizeRequest.newBuilder()
                    .setAudioContent(ByteString.copyFrom(payload))
                    .build());
        }
    }

    
    @Override
protected void handleBinaryMessage(WebSocketSession session, BinaryMessage message) {
    ByteBuffer payload = message.getPayload();
    byte[] audioBytes = new byte[payload.remaining()];
    payload.get(audioBytes);

    // Check if audio is mostly silent
    if (isSilent(audioBytes)) {
        System.out.println("üîá Skipping silent audio.");
        return;
    }

    System.out.println("üîä Sending audio data: " + audioBytes.length + " bytes");
    
    if (activeSessions.containsKey(session.getId())) {
        ApiStreamObserver<StreamingRecognizeRequest> requestObserver = activeSessions.get(session.getId());
        requestObserver.onNext(StreamingRecognizeRequest.newBuilder()
                .setAudioContent(ByteString.copyFrom(audioBytes))
                .build());
    }
}

protected void handleBinaryMessageBestBeforeLatest(WebSocketSession session, BinaryMessage message) {
    ByteBuffer payload = message.getPayload();
    byte[] audioBytes = new byte[payload.remaining()];
    payload.get(audioBytes);

    // Check if audio is mostly silent
    if (isSilent(audioBytes)) {
        System.out.println("üîá Skipping silent audio.");
        return; // Do not send silent audio
    }

    try {
        AudioUtils.saveAudioChunk(audioBytes); // Save as WAV with 3-second accumulation
    } catch (IOException e) {
        e.printStackTrace();
    }

    System.out.println("üîä Sending audio data: " + audioBytes.length + " bytes");
    if (activeSessions.containsKey(session.getId())) {
        ApiStreamObserver<StreamingRecognizeRequest> requestObserver = activeSessions.get(session.getId());
        requestObserver.onNext(StreamingRecognizeRequest.newBuilder()
                .setAudioContent(ByteString.copyFrom(audioBytes))
                .build());
    }
}

    protected void handleBinaryMessageSkipSilent(WebSocketSession session, BinaryMessage message) {
        ByteBuffer payload = message.getPayload();
        byte[] audioBytes = new byte[payload.remaining()];
        payload.get(audioBytes);
    
        // Check if audio is mostly silent
        if (isSilent(audioBytes)) {
            System.out.println("üîá Skipping silent audio.");
            return; // Do not send silent audio
        }
    
        System.out.println("üîä Sending audio data: " + audioBytes.length + " bytes");

try {
    saveAudioToFile(audioBytes, "sent_audio_" + System.currentTimeMillis() + ".raw");
} catch (IOException e) {
    e.printStackTrace();
}

        if (activeSessions.containsKey(session.getId())) {
            ApiStreamObserver<StreamingRecognizeRequest> requestObserver = activeSessions.get(session.getId());
            requestObserver.onNext(StreamingRecognizeRequest.newBuilder()
                    .setAudioContent(ByteString.copyFrom(audioBytes))
                    .build());
        }
    }
    
    
    private boolean isSilent(byte[] audioData) {
        int silenceThreshold = 250; // üîΩ Lower sensitivity (was 500)
        int countLowAmplitude = 0;
        
        for (byte b : audioData) {
            if (Math.abs(b) < 15) { // üîΩ Relax the threshold (was 10)
                countLowAmplitude++;
            }
        }
        
        return countLowAmplitude > (audioData.length * 0.90); // üîΩ Less aggressive cutoff (was 95%)
    }
    
    private boolean isSilent0(byte[] audioData) {
        int silenceThreshold = 500; // Adjust this threshold for sensitivity
        int countLowAmplitude = 0;
        
        for (byte b : audioData) {
            if (Math.abs(b) < 10) { // Amplitude below threshold
                countLowAmplitude++;
            }
        }
        
        return countLowAmplitude > (audioData.length * 0.95); // If 95% of samples are low, assume silence
    }
    

// @Override
protected void handleBinaryMessageSaveAudio(WebSocketSession session, BinaryMessage message) {
    ByteBuffer payload = message.getPayload();
    byte[] audioBytes = new byte[payload.remaining()];
    payload.get(audioBytes);

    try {
        saveAudioToFile(audioBytes, "audio_chunk_" + System.currentTimeMillis() + ".raw");
    } catch (IOException e) {
        e.printStackTrace();
    }

    System.out.println("üîä Sending audio data: " + audioBytes.length + " bytes");
    if (activeSessions.containsKey(session.getId())) {
        ApiStreamObserver<StreamingRecognizeRequest> requestObserver = activeSessions.get(session.getId());
        requestObserver.onNext(StreamingRecognizeRequest.newBuilder()
                .setAudioContent(ByteString.copyFrom(audioBytes))
                .build());
    }
}

// **Helper Function to Save Audio**
private void saveAudioToFile(byte[] audioData, String filename) throws IOException {
    String filePath = "C:/Users/opc/Downloads/audio_logs/" + filename; // Change path if needed
    Files.createDirectories(Paths.get("C:/Users/opc/Downloads/audio_logs/"));
    try (FileOutputStream fos = new FileOutputStream(filePath)) {
        fos.write(audioData);
    }
    System.out.println("üíæ Saved audio to: " + filePath);
}




    @Override
    public void afterConnectionClosed(WebSocketSession session, CloseStatus status) {
        System.out.println("üî¥ WebSocket Closed: " + session.getId());
        if (activeSessions.containsKey(session.getId())) {
            activeSessions.get(session.getId()).onCompleted();
            activeSessions.remove(session.getId());
        }
    }
}
